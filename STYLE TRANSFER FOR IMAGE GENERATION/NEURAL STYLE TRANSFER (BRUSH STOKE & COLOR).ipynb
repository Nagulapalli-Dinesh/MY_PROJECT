{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Jmwq_N9TuNlP"},"outputs":[],"source":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"mOaxaPSaW4tu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H27OXzrbY07S"},"source":[]},{"cell_type":"markdown","metadata":{"id":"a3ICoI9OHxzt"},"source":["# New Section"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eU_nlXd2ddd8"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/MyDrive')"]},{"cell_type":"markdown","metadata":{"id":"sYoKomAcmmwK"},"source":["# COLOUR NEURAL STYLE TRANSFER"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"10kMyAWbJLnGbBXygubnUz2jLA0F0QjSH"},"id":"WdH05CQsS_iS","executionInfo":{"status":"ok","timestamp":1739687324238,"user_tz":-330,"elapsed":1065038,"user":{"displayName":"chandu dinesh","userId":"14063215155313455902"}},"outputId":"26a77132-66b9-4b01-f55a-7bf19813137f"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import torch\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision import models, transforms\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","# ==================================\n","# 1. Load and Preprocess Images\n","# ==================================\n","def load_image(image_path, image_size):\n","    \"\"\"Loads an image, resizes it, and converts to tensor.\"\"\"\n","    transform = transforms.Compose([\n","        transforms.Resize(image_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","    image = Image.open(image_path).convert(\"RGB\")\n","    image = transform(image).unsqueeze(0)  # Add batch dimension\n","    return image\n","\n","# Set device (GPU if available)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Set image size\n","image_size = (512, 512)  # Change for larger images\n","\n","# Content Image Path\n","content_img_path = r\"/content/drive/MyDrive/Colab Notebooks/MY_PROJECT/IMAZE/content.png\"\n","style_img_path = r\"/content/drive/MyDrive/Colab Notebooks/MY_PROJECT/IMAZE/style.png\"\n","\n","# Load images\n","content_img = load_image(content_img_path, image_size).to(device)\n","style_img = load_image(style_img_path, image_size).to(device)\n","\n","# ==================================\n","# 2. Define VGG-19 Model\n","# ==================================\n","vgg = models.vgg19(pretrained=True).features.to(device).eval()\n","\n","# Define feature layers for content and style\n","vgg_layer_map = {\n","    \"conv1_1\": 0, \"conv2_1\": 5, \"conv3_1\": 10,\n","    \"conv4_1\": 19, \"conv4_2\": 21, \"conv5_1\": 28\n","}\n","\n","content_feature_layer = \"conv4_2\"\n","style_feature_layers = [\"conv1_1\", \"conv2_1\", \"conv3_1\", \"conv4_1\", \"conv5_1\"]  # âœ… Full style transfer\n","\n","content_feature_layer_idx = vgg_layer_map[content_feature_layer]\n","style_feature_layer_idxs = [vgg_layer_map[layer] for layer in style_feature_layers]\n","\n","# ==================================\n","# 3. Extract Features\n","# ==================================\n","def get_features(image, model, layers):\n","    \"\"\"Extracts features from VGG-19 for specified layers.\"\"\"\n","    features = {}\n","    x = image\n","    for name, layer in model.named_children():\n","        x = layer(x)\n","        if int(name) in layers:\n","            features[name] = x\n","    return features\n","\n","# Extract content and style features\n","content_features = get_features(content_img, vgg, [content_feature_layer_idx])\n","style_features = get_features(style_img, vgg, style_feature_layer_idxs)\n","\n","# ==================================\n","# 4. Define Loss Functions\n","# ==================================\n","def gram_matrix(tensor):\n","    \"\"\"Computes the Gram matrix for style representation.\"\"\"\n","    B, C, H, W = tensor.size()\n","    tensor = tensor.view(C, H * W)\n","    gram = torch.mm(tensor, tensor.t())\n","    return gram / (C * H * W)\n","\n","def compute_losses(dl_transfer, content_features, style_features, alpha, beta):\n","    \"\"\"Computes content and style loss.\"\"\"\n","    transfer_features = get_features(dl_transfer, vgg, [content_feature_layer_idx])\n","    transfer_style_features = get_features(dl_transfer, vgg, style_feature_layer_idxs)\n","\n","    # Compute content loss (preserves structure)\n","    content_loss = F.mse_loss(\n","        transfer_features[str(content_feature_layer_idx)],\n","        content_features[str(content_feature_layer_idx)]\n","    )\n","\n","    # Compute style loss (captures brush strokes & textures)\n","    style_loss = 0\n","    for layer_idx in style_feature_layer_idxs:\n","        transfer_gram = gram_matrix(transfer_style_features[str(layer_idx)])\n","        style_gram = gram_matrix(style_features[str(layer_idx)])\n","        style_loss += F.mse_loss(transfer_gram, style_gram)\n","\n","    total_loss = alpha * content_loss + beta * style_loss\n","    return content_loss, style_loss, total_loss\n","\n","# ==================================\n","# 5. Initialize Transfer Image\n","# ==================================\n","transfer_img = content_img.clone().requires_grad_()\n","\n","# ==================================\n","# 6. Training Loop\n","# ==================================\n","alpha = 1.0  # Content weight\n","beta = 1e5   # Style (texture) weight - increase for more stylization\n","num_iterations = 3000\n","learning_rate = 3.0\n","\n","# Adam optimizer\n","optimizer = optim.Adam([transfer_img], lr=learning_rate)\n","\n","# Track minimum loss\n","min_loss = float('inf')\n","best_transfer_image = None\n","\n","# Training loop\n","for iteration in range(1, num_iterations + 1):\n","\n","    # Compute losses\n","    optimizer.zero_grad()\n","    loss_content, loss_style, total_loss = compute_losses(transfer_img, content_features, style_features, alpha, beta)\n","\n","    # Backpropagate and update image\n","    total_loss.backward(retain_graph=True) # Add retain_graph=True here\n","    optimizer.step()\n","\n","    # Save the best image (lowest loss)\n","    if total_loss.item() < min_loss:\n","        min_loss = total_loss.item()\n","        best_transfer_image = transfer_img.clone()\n","\n","    # Display the stylized image every 100 iterations\n","    if iteration == 1 or iteration % 100 == 0:\n","        with torch.no_grad():\n","            transfer_image_np = transfer_img.squeeze().cpu().detach().numpy()\n","            transfer_image_np = transfer_image_np.transpose(1, 2, 0)  # Convert (C, H, W) to (H, W, C)\n","            transfer_image_np = transfer_image_np.clip(0, 1)  # Normalize\n","\n","            plt.imshow(transfer_image_np)\n","            plt.title(f\"Style Transfer After {iteration} Iterations\")\n","            plt.axis(\"off\")\n","            plt.show()\n","\n","# ==================================\n","# 7. Save Final Image\n","# ==================================\n","final_image = best_transfer_image.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n","final_image = final_image.clip(0, 1)\n","\n","plt.figure()\n","plt.imshow(final_image)\n","plt.title(\"Final Stylized Image\")\n","plt.axis(\"off\")\n","plt.show()\n","plt.savefig(r'/content/drive/MyDrive/Colab Notebooks/MY_PROJECT/IMAZE/final_image.png')\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"mount_file_id":"10wIDjNaY41OV7NsCtANurTvRnoHuVlVg","authorship_tag":"ABX9TyPEDmJFaQRQUriXdQXB24y4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}